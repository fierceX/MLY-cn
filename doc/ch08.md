## 使用单值评估指标进行优化


所谓的**单值评估指标（single-number evaluation metric）**有很多，分类准确率就是其中的一种：待你在开发集（或测试集）上运行分类器之后，它将返回单个数值，代表着样本被正确分类的比例。根据这个指标，如果分类器 A 的准确率为 97％，而分类器 B 的准确率为 90%，那么我们可以认为分类器 A 更优秀。

相比之下，**查准率**（Precision，又译作精度）和**查全率**（Recall，又译作召回率）的组合并不能作为单值评估指标，因为它给出了两个值来对你的分类器进行评估。多值评估指标提高了在算法之间进行优劣比较的难度，假设你的算法表现如下：

| Classifier | Precision | Recall |
| ---------- | --------- | ------ |
| A          | 95%       | 90%    |
| B          | 98%       | 85%    |

> 猫分类器的查准率指的是在开发集（或测试集）内，那些已经被预测为“猫”的图片之中，实际类别是“猫”的样本比例。而查全率指的是在开发集（或测试集）内，所有实际类别为“猫”的图片中，被正确预测为“猫”的样本比例。人们常常在查准率和查全率之间权衡取舍。

若根据上方表格中的数值对两个分类器进行比较，显然二者都没有较为明显的优势，因此也无法指导你立即做出选择。

| Classifier | Precision | Recall | F1 score  |
| ---------- | --------- | ------ | --------- |
| A          | 95%       | 90%    | **92.4%** |

当你的团队在进行开发时，往往会尝试多种多样的算法架构、模型参数、特征选择，或是一些其它的想法。你可以通过使用单值评估指标（如准确率），根据所有的模型在此指标上的表现，进行排序，从而能够快速确定哪一个模型的性能表现最好。

如果你认为查准率和查全率指标很关键，可以参照其他人的做法，将这两个值合并为一个值来表示。例如取二者的平均值，或者你可以计算 “F1分数（F1 score）” ，这是一种经过修正的平均值计算方法，比起直接取平均值的效果会好一些。

| Classifier | Precision | Recall | F1 score  |
| ---------- | --------- | ------ | --------- |
| A          | 95%       | 90%    | **92.4%** |
| B          | 98%       | 85%    | **91.0%** |

> 如果你想了解更多关于 F1 分数的信息，可以参考  <https://en.wikipedia.org/wiki/F1_score> 它是查准率和查全率的调和平均数，计算公式为 2 / ( (1/Precision) + (1/Recall) ).

综上可知，当你需要在多个分类器之间进行选择时，使用单值评估指标将帮助你更快速地作出决定。它能给出一个清晰明了的分类器性能排名，从而帮助团队明确后续的改进方向。

最后补充一个例子，假设你在 “美国” 、 “印度” 、 “中国” 和 “其它地区” 这四个关键市场追踪你的猫分类器准确率，并且获得了四个指标。通过对这四个指标取平均值或进行加权平均，你将得到一个单值指标。**取平均值或者加权平均值是将多个指标合并为一个指标的最常用方法之一。**
